{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners."
      ],
      "metadata": {
        "id": "xEgySjHVfAX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Boosting is an ensemble learning technique that combines multiple weak learners (usually decision trees) to create a strong learner that performs better than any single model alone.\n",
        "\n",
        "A weak learner is a model that performs slightly better than random guessing (for example, 55–60% accuracy). Boosting helps these weak learners “boost” their accuracy by training them sequentially, each one focusing on the errors made by the previous ones."
      ],
      "metadata": {
        "id": "5LROXRfgfhZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?"
      ],
      "metadata": {
        "id": "y9cYO2d3fkSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Both AdaBoost and Gradient Boosting are Boosting algorithms — they combine weak learners (usually decision trees) to form a strong learner."
      ],
      "metadata": {
        "id": "TKGHp6k2fmEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: How does regularization help in XGBoost?\n"
      ],
      "metadata": {
        "id": "PEx6zsl4fyof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> XGBoost (Extreme Gradient Boosting) is an advanced version of gradient boosting that includes regularization to make the model more robust, generalized, and less prone to overfitting."
      ],
      "metadata": {
        "id": "ykGSmj8JfzQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Why is CatBoost considered efficient for handling categorical data?\n"
      ],
      "metadata": {
        "id": "DPtcI8DUgEna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> CatBoost (short for Categorical Boosting) is a gradient boosting algorithm developed by Yandex, and it is specifically designed to handle categorical features efficiently and automatically — unlike most other boosting algorithms that require manual encoding."
      ],
      "metadata": {
        "id": "TIuAejGQgSSN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n"
      ],
      "metadata": {
        "id": "BBh_P-GtgVYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Both bagging (like Random Forest) and boosting (like AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost) are ensemble learning techniques, but they serve different purposes.\n",
        "\n",
        "Bagging reduces variance (good for high-variance models),\n",
        "Boosting reduces bias (good for weak models that underfit)."
      ],
      "metadata": {
        "id": "mC76J7nGgWEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy"
      ],
      "metadata": {
        "id": "WXNHtrpSgwh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = AdaBoostClassifier(n_estimators=100, learning_rate=0.8, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxQxNmWrg1wl",
        "outputId": "02e3596d-e247-445a-927b-23db9c77d308"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 96.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score"
      ],
      "metadata": {
        "id": "vS0ziD2AhEAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzrLaF5chHjN",
        "outputId": "8119dd48-b26e-47a7-cbae-5fad8ef7ec8e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.8004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy\n"
      ],
      "metadata": {
        "id": "XXkZoJa7hPUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_ * 100:.2f}%\")\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEUlY8OjhSfl",
        "outputId": "660c387c-8568-4028-fb81-783bc2f5aa83"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Best Cross-Validation Accuracy: 96.70%\n",
            "Test Set Accuracy: 95.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [10:58:58] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    }
  ]
}